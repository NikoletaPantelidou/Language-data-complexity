{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1IqrvuOQLxXTFhecD4i74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikoletaPantelidou/Language-data-complexity/blob/main/Language_complexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "ParameterTable = pd.read_csv(\"https://raw.githubusercontent.com/NikoletaPantelidou/Language-data-complexity/refs/heads/main/parameters.csv\")\n",
        "ValueTable = pd.read_csv(\"https://raw.githubusercontent.com/NikoletaPantelidou/Language-data-complexity/refs/heads/main/values.csv\" , on_bad_lines='skip')\n"
      ],
      "metadata": {
        "id": "kEvMScr-JksE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def make_binary_ParameterTable(ParameterTable, keep_multi_state_features=True, keep_raw_binary=False):\n",
        "    binarised_feats = [\n",
        "        \"GB024a\", \"GB024b\", \"GB025a\", \"GB025b\", \"GB065a\", \"GB065b\",\n",
        "        \"GB130a\", \"GB130b\", \"GB193a\", \"GB193b\", \"GB203a\", \"GB203b\"\n",
        "    ]\n",
        "\n",
        "    if keep_raw_binary and all(feat in ParameterTable['ID'].values for feat in binarised_feats):\n",
        "        return ParameterTable.copy()\n",
        "\n",
        "    parameter_binary = pd.DataFrame({\n",
        "        \"ID\": [\"GB024\", \"GB024\", \"GB025\", \"GB025\", \"GB065\", \"GB065\",\n",
        "               \"GB130\", \"GB130\", \"GB193\", \"GB193\", \"GB203\", \"GB203\"],\n",
        "        \"ID_binary\": binarised_feats,\n",
        "        \"Grambank_ID_desc_binary\": [\n",
        "            \"GB024a NUMOrder_Num-N\", \"GB024b NUMOrder_N-Num\",\n",
        "            \"GB025a DEMOrder_Dem-N\", \"GB025b DEMOrder_N-Dem\",\n",
        "            \"GB065a POSSOrder_PSR-PSD\", \"GB065b POSSOrder_PSD-PSR\",\n",
        "            \"GB130a IntransOrder_SV\", \"GB130b IntransOrder_VS\",\n",
        "            \"GB193a ANMOrder_ANM-N\", \"GB193b ANMOrder_N-ANM\",\n",
        "            \"GB203a UQOrder_UQ-N\", \"GB203b UQOrder_N-UQ\"\n",
        "        ],\n",
        "        \"Name_binary\": [\n",
        "            \"Is the order of the numeral and noun Num-N?\",\n",
        "            \"Is the order of the numeral and noun N-Num?\",\n",
        "            \"Is the order of the adnominal demonstrative and noun Dem-N?\",\n",
        "            \"Is the order of the adnominal demonstrative and noun N-Dem?\",\n",
        "            \"Is the pragmatically unmarked order of adnominal possessor noun and possessed noun PSR-PSD?\",\n",
        "            \"Is the pragmatically unmarked order of adnominal possessor noun and possessed noun PSD-PSR?\",\n",
        "            \"Is the pragmatically unmarked order of S and V in intransitive clauses S-V?\",\n",
        "            \"Is the pragmatically unmarked order of S and V in intransitive clauses V-S?\",\n",
        "            \"Is the order of the adnominal property word (ANM) and noun ANM-N?\",\n",
        "            \"Is the order of the adnominal property word (ANM) and noun N-ANM?\",\n",
        "            \"Is the order of the adnominal collective universal quantifier (UQ) and noun UQ-N?\",\n",
        "            \"Is the order of the adnominal collective universal quantifier (UQ) and noun N-UQ?\"\n",
        "        ],\n",
        "        \"Word_Order_binary\": [0, 1, 0, 1, 0, 1, None, None, 0, 1, 0, 1],\n",
        "        \"Binary_Multistate\": [\"Binarised\"] * 12\n",
        "    })\n",
        "\n",
        "    multistate_features = {\"GB024\", \"GB025\", \"GB065\", \"GB130\", \"GB193\", \"GB203\"}\n",
        "\n",
        "    ParameterTable_new = ParameterTable.merge(parameter_binary, on=\"ID\", how=\"outer\")\n",
        "\n",
        "    ParameterTable_new[\"ID\"] = ParameterTable_new[\"ID_binary\"].combine_first(ParameterTable_new[\"ID\"])\n",
        "    ParameterTable_new[\"Name\"] = ParameterTable_new[\"Name_binary\"].combine_first(ParameterTable_new[\"Name\"])\n",
        "    ParameterTable_new[\"Grambank_ID_desc\"] = ParameterTable_new[\"Grambank_ID_desc_binary\"].combine_first(ParameterTable_new[\"Grambank_ID_desc\"])\n",
        "    ParameterTable_new[\"Word_Order\"] = ParameterTable_new[\"Word_Order_binary\"].combine_first(ParameterTable_new[\"Word_Order\"])\n",
        "\n",
        "    ParameterTable_new = ParameterTable_new.drop(columns=[\"ID_binary\", \"Name_binary\", \"Grambank_ID_desc_binary\", \"Word_Order_binary\"])\n",
        "\n",
        "    ParameterTable_new[\"Binary_Multistate\"] = ParameterTable_new[\"ID\"].apply(\n",
        "        lambda x: \"Multi\" if x in multistate_features else \"Binary\"\n",
        "    )\n",
        "\n",
        "    if not keep_multi_state_features:\n",
        "        ParameterTable_new = ParameterTable_new[~ParameterTable_new[\"ID\"].isin(multistate_features)]\n",
        "\n",
        "    return ParameterTable_new\n",
        "\n"
      ],
      "metadata": {
        "id": "E0PbDGDrapl5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def binarise_GBXXX_to_GBXXXa_without_zero(values):\n",
        "    if \"0\" in values.values:\n",
        "        raise ValueError(\"Feature contains zero-values which are not permitted.\")\n",
        "    return values.replace({\"1\": \"1\", \"2\": \"0\", \"3\": \"1\", \"?\": \"?\", np.nan: np.nan})\n",
        "\n",
        "def binarise_GBXXX_to_GBXXXb_without_zero(values):\n",
        "    if \"0\" in values.values:\n",
        "        raise ValueError(\"Feature contains zero-values which are not permitted.\")\n",
        "    return values.replace({\"1\": \"0\", \"2\": \"1\", \"3\": \"1\", \"?\": \"?\", np.nan: np.nan})\n",
        "\n",
        "def binarise_GBXXX_to_GBXXXa_with_zero(values):\n",
        "    return values.replace({\"0\": \"0\", \"1\": \"1\", \"2\": \"0\", \"3\": \"1\", \"?\": \"?\", np.nan: np.nan})\n",
        "\n",
        "def binarise_GBXXX_to_GBXXXb_with_zero(values):\n",
        "    return values.replace({\"0\": \"0\", \"1\": \"0\", \"2\": \"1\", \"3\": \"1\", \"?\": \"?\", np.nan: np.nan})\n",
        "\n",
        "def gb_recode(values, oldvariable, newvariable, func):\n",
        "    subset = values[values[\"Parameter_ID\"] == oldvariable].copy()\n",
        "    subset[\"Parameter_ID\"] = newvariable\n",
        "    subset[\"ID\"] = newvariable + \"-\" + subset[\"Language_ID\"]\n",
        "    subset[\"Value\"] = func(subset[\"Value\"])\n",
        "    subset[\"Code_ID\"] = subset[\"Parameter_ID\"] + \"-\" + subset[\"Value\"].astype(str)\n",
        "    return pd.concat([values, subset], ignore_index=True)\n",
        "\n",
        "def make_binary_ValueTable(values, keep_multistate=False, keep_raw_binary=True, trim_to_only_raw_binary=False):\n",
        "    multistate_parameters = [\"GB024\", \"GB025\", \"GB065\", \"GB130\", \"GB193\", \"GB203\"]\n",
        "    binary_parameters = [\"GB024a\", \"GB024b\", \"GB025a\", \"GB025b\", \"GB065a\", \"GB065b\", \"GB130a\", \"GB130b\", \"GB193a\", \"GB193b\", \"GB203a\", \"GB203b\"]\n",
        "\n",
        "    if trim_to_only_raw_binary:\n",
        "        values = values[~values[\"Parameter_ID\"].isin(multistate_parameters)]\n",
        "        if not any(values[\"Parameter_ID\"].isin(binary_parameters)):\n",
        "            raise ValueError(\"There is no raw binary coding at all.\")\n",
        "    else:\n",
        "        if not keep_raw_binary:\n",
        "            values = values[~values[\"Parameter_ID\"].isin(binary_parameters)]\n",
        "        else:\n",
        "            values_raw_binary = values[values[\"Parameter_ID\"].isin(binary_parameters)]\n",
        "\n",
        "        values = gb_recode(values, 'GB024', 'GB024a', binarise_GBXXX_to_GBXXXa_without_zero)\n",
        "        values = gb_recode(values, 'GB024', 'GB024b', binarise_GBXXX_to_GBXXXb_without_zero)\n",
        "        values = gb_recode(values, 'GB025', 'GB025a', binarise_GBXXX_to_GBXXXa_without_zero)\n",
        "        values = gb_recode(values, 'GB025', 'GB025b', binarise_GBXXX_to_GBXXXb_without_zero)\n",
        "        values = gb_recode(values, 'GB065', 'GB065a', binarise_GBXXX_to_GBXXXa_without_zero)\n",
        "        values = gb_recode(values, 'GB065', 'GB065b', binarise_GBXXX_to_GBXXXb_without_zero)\n",
        "        values = gb_recode(values, 'GB130', 'GB130a', binarise_GBXXX_to_GBXXXa_without_zero)\n",
        "        values = gb_recode(values, 'GB130', 'GB130b', binarise_GBXXX_to_GBXXXb_without_zero)\n",
        "        values = gb_recode(values, 'GB193', 'GB193a', binarise_GBXXX_to_GBXXXa_with_zero)\n",
        "        values = gb_recode(values, 'GB193', 'GB193b', binarise_GBXXX_to_GBXXXb_with_zero)\n",
        "        values = gb_recode(values, 'GB203', 'GB203a', binarise_GBXXX_to_GBXXXa_with_zero)\n",
        "        values = gb_recode(values, 'GB203', 'GB203b', binarise_GBXXX_to_GBXXXb_with_zero)\n",
        "\n",
        "        if keep_raw_binary:\n",
        "            values = values[~values.set_index([\"Language_ID\", \"Parameter_ID\"]).index.isin(values_raw_binary.set_index([\"Language_ID\", \"Parameter_ID\"]).index)]\n",
        "            values = pd.concat([values, values_raw_binary], ignore_index=True)\n",
        "\n",
        "        if not keep_multistate:\n",
        "            values = values[~values[\"Parameter_ID\"].isin(multistate_parameters)]\n",
        "\n",
        "    return values\n"
      ],
      "metadata": {
        "id": "EdgghaHIbfMe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_fusion_scores(parameters_url, values_url):\n",
        "    # Load the data from the provided URLs\n",
        "    parameters = pd.read_csv(parameters_url)\n",
        "    values = pd.read_csv(values_url,on_bad_lines='skip', sep=',')\n",
        "    if \"GB203b\" not in values[\"Parameter_ID\"].values:\n",
        "        values = make_binary_ValueTable(values)\n",
        "\n",
        "    if \"GB203b\" not in parameters[\"ID\"].values:\n",
        "         parameters = make_binary_ParameterTable(parameters)\n",
        "\n",
        "\n",
        "\n",
        "    # Filter parameters to include only those with Fusion weight of 1\n",
        "    fusion_params = parameters[parameters['Fusion'] == 1]\n",
        "\n",
        "    # Merge values with fusion parameters to focus only on relevant features\n",
        "    merged_data = values.merge(fusion_params, left_on='Parameter_ID', right_on='ID')\n",
        "    merged_data['Value'] = pd.to_numeric(merged_data['Value'], errors='coerce')\n",
        "\n",
        "    # Create a pivot table with languages as rows and parameters as columns\n",
        "    pivot_table = merged_data.pivot_table(index='Language_ID', columns='Parameter_ID', values='Value', aggfunc='first')\n",
        "\n",
        "    # Calculate the percentage of missing data for each language\n",
        "    missing_data_percentage = pivot_table.isnull().mean(axis=1)\n",
        "\n",
        "    # Increase threshold to allow more missing data if necessary\n",
        "    filtered_languages = pivot_table[missing_data_percentage <= 0.5]  # Adjusted threshold to 50%\n",
        "\n",
        "    # Calculate the percentage of missing data for each language\n",
        "    missing_data_percentage = pivot_table.isnull().mean(axis=1)\n",
        "\n",
        "    # Filter out languages with more than 60% missing data\n",
        "    filtered_languages = pivot_table[missing_data_percentage <= 0.6]\n",
        "\n",
        "    # Calculate the Fusion score as the mean of available (non-missing) values for each language\n",
        "    fusion_scores = filtered_languages.mean(axis=1)\n",
        "\n",
        "    return fusion_scores\n",
        "\n",
        "# URLs of the CSV files\n",
        "parameters_url = 'https://raw.githubusercontent.com/NikoletaPantelidou/Language-data-complexity/refs/heads/main/parameters.csv'\n",
        "values_url ='https://raw.githubusercontent.com/NikoletaPantelidou/Language-data-complexity/refs/heads/main/values.csv'\n",
        "\n",
        "# Calculate Fusion scores\n",
        "fusion_scores = calculate_fusion_scores(parameters_url, values_url)\n",
        "\n",
        "# Display the Fusion scores\n",
        "print(fusion_scores)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3khMY8GX-NC",
        "outputId": "8aa275fb-9ba0-4ca8-d9c6-6ed54fe571f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language_ID\n",
            "mode1248    0.538462\n",
            "stan1288    0.400000\n",
            "stan1289    0.418182\n",
            "stan1293    0.291667\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_informativity(values_url, parameters_url):\n",
        "\n",
        "\n",
        "    # Load the data from the provided URLs\n",
        "    parameters = pd.read_csv(parameters_url)\n",
        "    values = pd.read_csv(values_url,on_bad_lines='skip', sep=',' )\n",
        "    # Merge ValueTable with Informativity categories from ParameterTable\n",
        "\n",
        "    parameters.columns = parameters.columns.str.strip()\n",
        "\n",
        "# Merge using correct column names\n",
        "    values = values.merge(parameters[['ID', 'Informativity']],\n",
        "                      left_on=\"Parameter_ID\", right_on=\"ID\",\n",
        "                      how=\"left\")\n",
        "\n",
        "    # Drop redundant \"ID\" column after merging\n",
        "    # The merge creates 'ID_x', 'ID_y' columns. Drop 'ID_y' which came from parameters DataFrame.\n",
        "    # values = values.drop(columns=[\"ID\"])  # Original line causing the error\n",
        "    values = values.drop(columns=['ID_y']) # Drop the 'ID_y' column instead of 'ID'\n",
        "    # Rename the \"ID_x\" column back to \"ID\" for consistency\n",
        "    values = values.rename(columns={'ID_x': 'ID'})\n",
        "\n",
        "# Drop redundant \"ID\" column after merging\n",
        "    values = values.drop(columns=[\"ID\"])  # Ignore if not present\n",
        "\n",
        "\n",
        "    # Drop missing Informativity values (features without a category are ignored)\n",
        "    values = values.dropna(subset=['Informativity'])\n",
        "\n",
        "    # Convert values to numeric, replacing \"?\" or invalid entries with NaN\n",
        "    values['Value'] = pd.to_numeric(values['Value'], errors='coerce') # Changed 'value' to 'Value'\n",
        "\n",
        "    # Filter out rows where value is NaN (missing data)\n",
        "    values = values.drop(columns=[\"ID\"], errors=\"ignore\")\n",
        "\n",
        "    # Step 1: Check if a language has at least one '1' in each Informativity group\n",
        "    informativity_check = values.groupby([\"Language_ID\", \"Informativity\"])[\"Value\"].max().reset_index()\n",
        "\n",
        "    # Step 2: If max value in a group is 1, it counts as marked\n",
        "    informativity_check[\"Marked\"] = (informativity_check[\"Value\"] == 1).astype(int)\n",
        "\n",
        "    # Step 3: Count marked categories per language\n",
        "    marked_counts = informativity_check.groupby(\"Language_ID\")[\"Marked\"].sum().reset_index(name=\"Marked_Categories\")\n",
        "\n",
        "    # Step 4: Count total relevant categories per language (groups with at least one feature)\n",
        "    total_counts = informativity_check.groupby(\"Language_ID\")[\"Informativity\"].nunique().reset_index(name=\"Total_Categories\")\n",
        "\n",
        "    # Step 5: Compute informativity score = (Marked_Categories / Total_Categories)\n",
        "    result_df = marked_counts.merge(total_counts, on=\"Language_ID\")\n",
        "    result_df[\"Informativity\"] = result_df[\"Marked_Categories\"] / result_df[\"Total_Categories\"]\n",
        "\n",
        "    return result_df\n",
        "\n"
      ],
      "metadata": {
        "id": "tswTqL7Yc5UG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_url = 'https://raw.githubusercontent.com/NikoletaPantelidou/Language-data-complexity/refs/heads/main/parameters.csv'\n",
        "values_url = 'https://raw.githubusercontent.com/NikoletaPantelidou/Language-data-complexity/refs/heads/main/values.csv'\n",
        "informativity_scores = calculate_informativity(values_url, parameters_url)\n",
        "\n",
        "# Print full table\n",
        "print(informativity_scores[0:])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LR27I8TeFv-",
        "outputId": "3207c704-5fff-4934-adad-4d55d97534dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Language_ID  Marked_Categories  Total_Categories  Informativity\n",
            "0    mode1248                 22                49       0.448980\n",
            "1    stan1288                 23                54       0.425926\n",
            "2    stan1289                 22                52       0.423077\n",
            "3    stan1293                  8                28       0.285714\n"
          ]
        }
      ]
    }
  ]
}